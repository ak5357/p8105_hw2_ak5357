---
title: "Data Science Homework 2"
author: "ak5357"
date: "2024-09-26"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(dplyr)
```

# Problem 1
### _NYC Transit Data_

**Loading and cleaning the data, keeping only the following variables:**

* Line
* Station Name
* Station Latitude and Longitude
* Routes Served
* Entry
* Vending
* Entrance Type
* ADA Compliance

```{r import_subway_data, message = FALSE}
transit_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", #load data
           na = c("NA", " ", "")) |> #define possible NA values
  janitor::clean_names() |> #standardize column names
  select(line, starts_with("station"), -station_location, #select focus columns
         starts_with("route"), entry, vending, entrance_type, ada) |> 
  mutate(
    entry = case_match( #convert entry column from char to lgl
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE,
    )) |> 
  mutate( #ensure all route columns are char type for consistency
    across(
      starts_with("route"), as.character
    ))
```

The dataset contains information related to each entrance and exit for each subway station in New York City. (Since this question doesn't count for points, I will forego further explanation about the dataset's variables. Sorry <3) The data are not tidy, since there is significant redundance among the columns, specifically those related to the routes.

**Answering questions with data.**
```{r data_questions}
# How many distinct stations?
num_distinct_stations = transit_df |> 
  distinct(line, station_name) |> 
  nrow()

# How many ADA stations?
num_ada_entr_ext = transit_df |>
  filter(ada == TRUE) |> 
  nrow()

# How many entrances/exits without vending and with entry?
p_no_vending_yes_entry = transit_df |> 
  summarize(
    proportion = mean(vending == "NO" & entry == TRUE)
    ) |> 
  round(4)
```

Here are some interesting facts about the data:

* The dataset contains information about **`r num_distinct_stations`** distinct stations.
* Of the `r nrow(transit_df)` entrances and exists documented in the dataset, only **`r num_ada_entr_ext`** are ADA compliant.
* The proportion of station entrances and exits without vending that allow entrance is **`r p_no_vending_yes_entry`**, or **`r p_no_vending_yes_entry * 100`**%.

**Reformatting data so that route number and route name are distinct variables.**
```{r reformat_df_by_route}
transit_dr_by_route = transit_df |> 
  pivot_longer(
    cols = starts_with("route"),
    names_to = "route_number",
    values_to = "route_name",
    names_prefix = "route"
  ) |> 
  filter(!is.na(route_name))
```


**Answering more questions with data.**
```{r _distinct_A_stations}
# How many distinct stations serve the A-line?
n_distinct_A_stations = transit_dr_by_route |> 
  filter(route_name == "A")  |> 
  distinct(line, station_name) |> 
  nrow()

# How many A-line stations are ADA compliant?
n_ada_A_stations = transit_dr_by_route |> 
  filter(route_name == "A" & ada == TRUE) |> 
  distinct(line, station_name) |> 
  nrow()
```

Here are some more interesting facts about the data:

* There are **`r n_distinct_A_stations`** distinct stations that serve the A train.
* Of the stations that serve the A train, only **`r n_ada_A_stations`** are ADA compliant.


# Problem 2
### _Mr. Trash Wheel_

**Loading and cleaning the data.**

Writing a function below to read an excel sheet into a dataframe, given the parameters `filepath` and `sheet_name`. Hopefully this will minimize errors by avoiding redundant code and standardize the loading/cleaning process across all trash wheels' excel sheets.

```{r read_excel_sheet_function}
sheet_to_df = function(filepath, sheet_name=NULL){
  output = read_excel(
                    path = filepath,
                    sheet = sheet_name,
                    na = c("NA", " ", ""),
                    skip = 1,
                    trim_ws = TRUE
                    ) |> 
    janitor::clean_names() |> #clean column names
    select(where(~ any(!is.na(.)))) #remove any columns with only NA values
  return(output)
}
```


I'm choosing to handle the alteration, removal, and addition of specific columns outside of the `sheet_to_df` function to keep the function as minimal and generalizable as possible.
```{r load_trash_wheels_data, message = FALSE}
# Assigning trash wheel excel filepath string to a variable since it's repeated
trash_excel_filepath = "data/202409 Trash Wheel Collection Data.xlsx"

# Mr. Trash Wheel
mr_trash_df = sheet_to_df(trash_excel_filepath, "Mr. Trash Wheel") |>
  mutate(
    sports_balls = as.integer(sports_balls), #round sports balls column
    year = as.numeric(year), #ensure the year data type is dbl
    wheel_name = "Mr. Trash Wheel" #add name column
    )

# Professor Trash Wheel
professor_trash_df = sheet_to_df(trash_excel_filepath, "Professor Trash Wheel") |> 
  filter(dumpster != 119) |> #removing extra row at the bottom
  mutate(wheel_name = "Professor Trash Wheel") #add name column

# Gwynnda Trash Wheel
gwynnda_trash_df = sheet_to_df(trash_excel_filepath, "Gwynnda Trash Wheel") |>
  mutate(wheel_name = "Gwynnda Trash Wheel") #add name column
```

**Combining the Trash Wheel dataframes.**

The following code chunk combines the three trash dataframes, while performing the following operations as well:

* Removing empty and "total" rows that came from the bottom of the original spreadsheets.
* Correcting the data types of `dumpster` and `wheel_name` to numeric and factor, respectively.
* Re-ordering the columns in a useful way, prioritizing the variables in this order: identifiers, temporal, quantifiers, other.

```{r combine_trash_dfs}
# Combine all trash wheels data
trash_wheels_df = 
  bind_rows(mr_trash_df, professor_trash_df, gwynnda_trash_df) |>
  filter(!is.na(dumpster)) |> #removing extra rows at the bottom
  mutate(
    dumpster = as.numeric(dumpster),
    wheel_name = as.factor(wheel_name)
  ) |> 
  relocate( #reorder variables
    wheel_name, dumpster, #wheel & dumpster identifiers
    year, month, date, #time-related
    weight_tons, volume_cubic_yards #quantity-related
    )
```


**Summarizing the data.**
```{r trash_n_observations, message = FALSE}
# Number of total observations
n_total_observations = nrow(trash_wheels_df)

# Number of observations from each wheel
n_mr_trash_observations = trash_wheels_df |> 
  filter(wheel_name == "Mr. Trash Wheel") |> 
  nrow()

n_professor_observations = trash_wheels_df |> 
  filter(wheel_name == "Professor Trash Wheel") |> 
  nrow()

n_gwynnda_observations = trash_wheels_df |> 
  filter(wheel_name == "Gwynnda Trash Wheel") |> 
  nrow()
```

The resulting dataset contains **`r nrow(trash_wheels_df)`** observations. Of these, **`r n_mr_trash_observations`** are from Mr. Trash Wheel, **`r n_professor_observations`** are from Professor Trash Wheel, and **`r n_gwynnda_observations`** are from Gwynnda Trash Wheel. The table below summarizes the following key variables by trash wheel:

* **Date of first collection:**
  * Earliest record for each trash wheel, `first_date`
* **Dumpsters visited:**
  * Count of distinct dumpsters were visited by each wheel, `dumpster_count`
* **Volume (cubic yards):**
  * Total volume collected, `total_vol`
  * Max volume collected at any one instance, `max_vol`
* **Weight (tons):**
  * Total weight collected, `total_weight`
  * Max weight collected at any one instance, `max_weight`

```{r trash_summary_key_variables}
# Summary table of key variables
trash_key_var_summary = trash_wheels_df |> 
  group_by(wheel_name) |> 
  summarise(
    first_date = min(date, na.rm = TRUE),
    dumpster_count = n_distinct(dumpster, na.rm = TRUE),
    total_volume = sum(volume_cubic_yards, na.rm = TRUE),
    max_volume = max(volume_cubic_yards, na.rm = TRUE),
    total_weight = sum(weight_tons, na.rm = TRUE),
    max_weight = max(weight_tons, na.rm = TRUE)
  ) |> 
  arrange(first_date)

# Reformat column names
colnames(trash_key_var_summary) = 
  str_to_title(str_replace_all(colnames(trash_key_var_summary), "_", " "))

# Output reader-friendly table
knitr::kable(trash_key_var_summary)
```

Other variables included in the dataset are litter types (ie. plastic bottles, polystyrene, cigarette butts, glass bottles, plastic bags, wrappers, sports balls) and homes powered. The following summary table shows sums of each litter type collected and sums of homes powered.

```{r trash_summary_other_variables}
# Summary table of other variables
trash_other_var_summary = trash_wheels_df |> 
  group_by(wheel_name) |> 
  summarise(
    first_date = min(date, na.rm = TRUE),
    total_plastic_bottles = sum(plastic_bottles, na.rm = TRUE),
    total_polystyrene = sum(polystyrene, na.rm = TRUE),
    total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE),
    total_glass_bottles = sum(glass_bottles, na.rm = TRUE),
    total_plastic_bags = sum(plastic_bags, na.rm = TRUE),
    total_wrappers = sum(wrappers, na.rm = TRUE),
    total_sports_balls = sum(sports_balls, na.rm = TRUE),
    total_homes_powered = sum(homes_powered, na.rm = TRUE)
  ) |> 
  arrange(first_date) |> 
  select(-first_date)

# Reformat column names
colnames(trash_other_var_summary) = 
  str_to_title(str_replace_all(colnames(trash_other_var_summary), "_", " "))

# Output reader-friendly table
knitr::kable(trash_other_var_summary)
```

**Answering questions with data.**
```{r trash_questions}
prof_trash_total_weight = trash_wheels_df |> 
  filter(wheel_name == "Professor Trash Wheel") |> 
  select(weight_tons) |> 
  sum(na.rm = TRUE)

gwynnda_june_2022_cigs = trash_wheels_df |> 
  filter(wheel_name == "Gwynnda Trash Wheel",
         month == "June",
         year == 2022
  ) |> 
  select(cigarette_butts) |> 
  sum(na.rm = TRUE)
```

Here are some fun facts about the data:

* In total, Professor Trash Wheel collected **`r prof_trash_total_weight`** tons of trash.
* In June 2022, Gwynnda Trash Wheel collected **`r format(gwynnda_june_2022_cigs, big.mark = ",")`** cigarette butts.

# Problem 3
### _Great British Bake Off_

**Loading the data.**

Information about individual bakers, their bakes, and their performance is included in `bakers.csv`, `bakes.csv`, and `results.csv`. The following code chunks import these three datasets, cleans and organizes them, and merges them into a single dataset containing all the information.

First, let's import the datasets.
```{r import_gbb_data, message = FALSE}
# Bakers data
bakers_df =
  read_csv("data/gbb_datasets/bakers.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()

# Bakes data
bakes_df =
  read_csv("data/gbb_datasets/bakes.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()

# Results data
results_df =
  read_csv("data/gbb_datasets/results.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE,
           skip = 2) |> 
  janitor::clean_names()
```


Now let's take a quick look at each dataframe.
```{r view_gbb_dfs, eval = FALSE}
# View each table
bakers_df |> view()
bakes_df |> view()
results_df |> view()
```

**Cleaning the data.**

Let's explore any `NA` values in the three dataframes.
```{r explore_gbb_na, eval = FALSE}
# Are there NA values in bakers_df?
bakers_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()

# Are there NA values in bakes_df?
bakes_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()

# Are there NA values in results_df?
results_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()
```

Based on the outputs of the code chunk above, we can see that

* `bakers_df` does not have any `NA` values
* `bakes_df` has `NA` values in the `show_stopper` column
* `results_df` has hundreds of `NA` values throughout the `technical` and `results` columns (possibly reflecting the null scores of previously eliminated contestants)

**QAQC for `bakes_df` and `results_df`.**

Before joining the three datasets, let's do some QA/QC to ensure the joins will go smoothly. First, let's compare the data in `bakes_df` and `results_df`.
```{r qaqc_bakes_results_1}
# Are there any bakers in bakes_df that are not in results_df?
bakes_df |> 
  anti_join(results_df, by = c("series", "episode", "baker")) |> 
  distinct(baker) |> 
  pull(baker)
```

It appears the contestant _"Jo"_ is present in `bakes_df` but not `results_df`. After some Googling, I see that _"Jo"_ in `bakes_df` is the same person as _Joanne_ in `results_df` and _Jo Wheatley_ in `bakers_df`. Let's standardize her name across all datasets to be _Joanne Wheatley_.
```{r qaqc_joanne}
# Mutate bakers_df
bakers_df = bakers_df |> 
  mutate(baker_name = recode(baker_name, "Jo Wheatley" = "Joanne Wheatley"))

# Mutate bakes_df
bakes_df = bakes_df |> 
  mutate(baker = recode(baker, "\"Jo\"" = "Joanne"))

# No need to mutate results_df
```

Let's continue the QA/QC.
```{r qaqc_bakes_results_2}
# Are there any bakers in results_df that are not in bakes_df?
results_df |> 
  anti_join(bakes_df, by = c("series", "episode", "baker")) |> 
  distinct(series, baker) |> 
  pull(baker)
```

There are many bakers in `results_df` that do not have a match with rows in `bakes_df`. After looking closer using the `view()` function, it appears that many (but not all) of these names are of previously eliminated contestants. Let's move forward with joining joining `bakes_df` and `results_df` and filter out unnecessary rows (null results of eliminated contestants) from there. The code below also mutates the `show_stopper` and `signature_bake` columns to improve readability wherever there are entries with multiple baked items.
```{r join_bakes_results}
# Full join on bakes_df and results_df
bakes_results_df = bakes_df |> 
  full_join(results_df, by = c("series", "episode", "baker")) |> 
  filter(
    !(is.na(signature_bake) & is.na(show_stopper) & is.na(technical) & is.na(result))
  ) |>
  arrange(series, baker, episode) |> 
  mutate( #improving readability wherever there are multiple dishes in one entry
    show_stopper = gsub("\n", " / ", show_stopper),
    signature_bake = gsub("\n", " / ", signature_bake),
    show_stopper = gsub("([a-z])([A-Z])", "\\1 / \\2", show_stopper),
    signature_bake = gsub("([a-z])([A-Z])", "\\1 / \\2", signature_bake),
    show_stopper = gsub("and /", "and", show_stopper),
    signature_bake = gsub("and /", "and", signature_bake),
    show_stopper = gsub("with /", "with", show_stopper),
    signature_bake = gsub("with /", "with", signature_bake),
    show_stopper = gsub("/ with", "with", show_stopper),
    signature_bake = gsub("/ with", "with", signature_bake),
    show_stopper = gsub("/ \\(", "\\(", show_stopper),
    signature_bake = gsub("/ \\(", "\\(", signature_bake)
  )

# Show first 10 rows
bakes_results_df |> head(10)
```

There are still some rows with missing data. Upon examining the output of the code chunk below, it seems there are two sources for these `NA` values.
```{r view_bakes_results_nas, eval = FALSE}
# Show all rows with any NA values
bakes_results_df |> 
  filter(if_any(everything(), is.na)) |> 
  arrange(series, episode) |> 
  view()
```

1. Series 1-8: There are some `NA` values here and there, wherever there is missing data for the `signature_bake` or `show_stopper` columns.
2. Series 9-10: There is no data at all for the `signature_bake` or `show_stopper` columns.
```{r bakes_results_series_difference}
# bakes_df only documents Series 1-8
bakes_df |> 
  distinct(series) |> 
  pull(series)

# results_df documents series 1-10
results_df |> 
  distinct(series) |> 
  pull(series)
```

Both of these missing data causes are out of my control, so let's move on with joining the datasets.

_Side note: For further QA/QC, I could check to make sure each baker has a singular show-ending (just one instance of "OUT", "Runner-up", or "WINNER") and that the episode where they have that result is the last one in their respective series where they have non-`NA` values. However, if I were to discover errors there, those would also be somewhat out of my control, so I don't see a point in going down that road._

**Prep `bakers_df` for joining with `bakes_results_df`.**

In `bakes_df` and `results_df` (now joined as `bakes_results_df`), the bakers' names are stored as just their first names. In `bakers_df`, the bakers' _full_ names are saved in a single column, so let's separate that `baker_name` column into two separate columns for first name and last name. This will make it easier to join `bakers_df` with `bakes_results_df` in the future.
```{r clean_bakers}
# Replace baker_name column with baker_firstname and baker_lastname columns
bakers_df = bakers_df |> 
  mutate(
    baker_firstname = str_split(baker_name, " ", simplify = TRUE)[, 1],
    baker_lastname = str_split(baker_name, " ", simplify = TRUE)[, 2]
  ) |> 
  select(-baker_name) |> 
  relocate(baker_firstname, baker_lastname)
```

**Finish joining the three datasets.**

I'm choosing to join the datasets in this order because I want the contestants' personal information columns to be at the end, since the focus here is on their baking.
```{r join_all_gbb}
gbb_df = bakes_results_df |> 
  full_join(bakers_df, by = c("series", "baker" = "baker_firstname")) |>
  rename(baker_firstname = baker) |> 
  arrange(series, baker_firstname, baker_lastname, episode) |> 
  relocate(series, episode, baker_firstname, baker_lastname)
```

**Export Final Dataset**

The final dataset contains information about all bakers, bakes, and results in Series 1-8 of the Great British Bake Off show. For Series 9-10, it contains information about bakers and results. Through the process detailed above, I have removed all entries representing previously eliminated characters, improved readability in the bakes columns, and sorted the columns and rows for better readability.
```{r}
# Export csv
write_csv(gbb_df, "data/gbb_datasets/gbb_processed_dataset.csv")
```

**Process summary.**

That was a long process. In my final code, I would do it as shown below, all in one go.
```{r gbb_data_wrangling_summary, eval = FALSE}
# Import Bakers data
bakers_df =
  read_csv("data/gbb_datasets/bakers.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()|> 
  mutate( #add firstname and lastname columns & fix Joanne naming issue
    baker_name = recode(baker_name, "Jo Wheatley" = "Joanne Wheatley"),
    baker_firstname = str_split(baker_name, " ", simplify = TRUE)[, 1],
    baker_lastname = str_split(baker_name, " ", simplify = TRUE)[, 2]
  ) |> 
  select(-baker_name) |> #remove redundant fullname column
  relocate(baker_firstname, baker_lastname) #place name columns at front

# Import Bakes data
bakes_df =
  read_csv("data/gbb_datasets/bakes.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names() |> 
  mutate(baker = recode(baker, "\"Jo\"" = "Joanne")) |> #fix Joanne naming issue 
  mutate( #improving readability wherever there are multiple dishes in one entry
    show_stopper = gsub("\n", " / ", show_stopper),
    signature_bake = gsub("\n", " / ", signature_bake),
    show_stopper = gsub("([a-z])([A-Z])", "\\1 / \\2", show_stopper),
    signature_bake = gsub("([a-z])([A-Z])", "\\1 / \\2", signature_bake),
    show_stopper = gsub("and /", "and", show_stopper),
    signature_bake = gsub("and /", "and", signature_bake),
    show_stopper = gsub("with /", "with", show_stopper),
    signature_bake = gsub("with /", "with", signature_bake),
    show_stopper = gsub("/ with", "with", show_stopper),
    signature_bake = gsub("/ with", "with", signature_bake),
    show_stopper = gsub("/ \\(", "\\(", show_stopper),
    signature_bake = gsub("/ \\(", "\\(", signature_bake)
  )

# Import Results data
results_df =
  read_csv("data/gbb_datasets/results.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE,
           skip = 2) |> 
  janitor::clean_names()

# Join dataframes
gbb_df = bakes_df |> 
  full_join(results_df, by = c("series", "episode", "baker")) |> 
  drop_na(result) |> #remove all rows that represent previously eliminated contestants
  full_join(bakers_df, by = c("series", "baker" = "baker_firstname")) |>
  rename(baker_firstname = baker) |> 
  arrange(series, baker_firstname, baker_lastname, episode) |> 
  relocate(series, episode, baker_firstname, baker_lastname)

# Export csv
write_csv(gbb_df, "data/gbb_datasets/gbb_processed_dataset.csv")
```

**Answering questions with data.**

The table below shows the star baker and winner of each episode and series. Since I have never watched the show, I can't speak to the baking talents or abilities or each contestant, so I'm not sure whose win is surprising and whose is not, but I will make some guesses anyway.

For surprise wins, I will say that Nancy Birtwhistle made quite a comeback in Series 1, earning the Star Baker result only once, in the very first episode, and then staying under the radar until the final episode where she emerged the Series 1 winner. David Atherton had a similar story in Series 10.

As for predictable overall winners, I would say that Nadia Hussain and Candice Brown from Series 6 and 7, respectively, "took the cake" for that title. In Series 6, Nadia dominated the Star Baker award for the two episodes leading up to the Series finale, so it's no shocker that she won that Series. Series 7's Candice Brown also performed consistently well, earning two Star Bakers before her final Win.
```{r gbb_winners_summary}
# Create table showing star baker/winner from each episode
gbb_winners_df = gbb_df |> 
  filter(
    result %in% c("STAR BAKER", "WINNER"), #keep only star baker/winner results
    series >= 5 #keep only series 5-10
    ) |>
  mutate(
    result = stringr::str_to_title(result)
  ) |> 
  select(series, episode, baker_firstname, baker_lastname, result) |> 
  arrange(series, episode)

# Reformat column names
colnames(gbb_winners_df) =
  str_to_title(str_replace_all(colnames(gbb_winners_df), "_", " "))

# Output reader-friendly table
knitr::kable(gbb_winners_df)
```

**Process viewership data.**

The following code chunk will import, clean, tidy, and organize the viewership data in `viewers.csv`.
```{r import_viewers_data, message = FALSE}
# Import, clean, and tidy viewership data
viewers_df = read_csv("data/gbb_datasets/viewers.csv") |> 
  janitor::clean_names() |> 
  pivot_longer( #tidy data from redundant series columns
    cols = starts_with("series"),
    names_to = "series",
    values_to = "rating",
    names_prefix = "series_"
  ) |> 
  mutate(series = as.numeric(series)) |> 
  relocate(series) |> 
  arrange(series, episode) |> #sort by series and episode
  filter(!is.na(rating)) #filter out non-existent episodes
```


```{r preview_viewers_data}
# Show the first 10 rows of viewers_df
viewers_df |> head(10)
```

**Answering questions with data.**
```{r gbb_view_summary}
# Viewership summary table
view_summary = viewers_df |> 
  group_by(series) |> 
  summarise(
    average_rating = round(mean(rating), 2),
    highest_episode = episode[which.max(rating)],
    highest_rating = max(rating)
  )

# View summary table
knitr::kable(view_summary)
```

```{r viewership_stats}
# Series 1 average viewership
s1_avg_viewership = view_summary |> 
  filter(series == 1) |> 
  pull(average_rating)

# Series 5 average viewership
s5_avg_viewership = view_summary |> 
  filter(series == 5) |> 
  pull(average_rating)
```

The table above shows the average viewership, highest-rated episode, and highest rating received by each season. The average viewership in Season 1 was **`r s1_avg_viewership`**. The average viewership in Season 5 was **`r s5_avg_viewership`**.