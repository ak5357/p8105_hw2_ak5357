---
title: "Data Science Homework 2"
author: "ak5357"
date: "2024-09-26"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(dplyr)
```

# Problem 1
### _NYC Transit Data_

**Loading and cleaning the data, keeping only the following variables:**

* Line
* Station Name
* Station Latitude and Longitude
* Routes Served
* Entry
* Vending
* Entrance Type
* ADA Compliance

```{r import_subway_data, message = FALSE}
transit_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", #load data
           na = c("NA", " ", "")) |> #define possible NA values
  janitor::clean_names() |> #standardize column names
  select(line, starts_with("station"), -station_location, #select focus columns
         starts_with("route"), entry, vending, entrance_type, ada) |> 
  mutate(
    entry = case_match( #convert entry column from char to lgl
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE,
    )) |> 
  mutate( #ensure all route columns are char type for consistency
    across(
      starts_with("route"), as.character
    ))
```

The dataset contains information related to each entrance and exit for each subway station in New York City. (Since this question doesn't count for points, I will forego further explanation about the dataset's variables. Sorry <3) The data are not tidy, since there is significant redundance among the columns, specifically those related to the routes.

**Answering questions with data.**
```{r data_questions}
# How many distinct stations?
num_distinct_stations = transit_df |> 
  distinct(line, station_name) |> 
  nrow()

# How many ADA stations?
num_ada_entr_ext = transit_df |>
  filter(ada == TRUE) |> 
  nrow()

# How many entrances/exits without vending and with entry?
p_no_vending_yes_entry = transit_df |> 
  summarize(
    proportion = mean(vending == "NO" & entry == TRUE)
    ) |> 
  round(4)
```

Here are some interesting facts about the data:

* The dataset contains information about **`r num_distinct_stations`** distinct stations.
* Of the `r nrow(transit_df)` entrances and exists documented in the dataset, only **`r num_ada_entr_ext`** are ADA compliant.
* The proportion of station entrances and exits without vending that allow entrance is **`r p_no_vending_yes_entry`**, or **`r p_no_vending_yes_entry * 100`**%.

**Reformatting data so that route number and route name are distinct variables.**
```{r reformat_df_by_route}
transit_dr_by_route = transit_df |> 
  pivot_longer(
    cols = starts_with("route"),
    names_to = "route_number",
    values_to = "route_name",
    names_prefix = "route"
  ) |> 
  filter(!is.na(route_name))
```


**Answering more questions with data.**
```{r _distinct_A_stations}
# How many distinct stations serve the A-line?
n_distinct_A_stations = transit_dr_by_route |> 
  filter(route_name == "A")  |> 
  distinct(line, station_name) |> 
  nrow()

# How many A-line stations are ADA compliant?
n_ada_A_stations = transit_dr_by_route |> 
  filter(route_name == "A" & ada == TRUE) |> 
  distinct(line, station_name) |> 
  nrow()
```

Here are some more interesting facts about the data:

* There are **`r n_distinct_A_stations`** distinct stations that serve the A train.
* Of the stations that serve the A train, only **`r n_ada_A_stations`** are ADA compliant.


# Problem 2
### _Mr. Trash Wheel_

**Loading and cleaning the data.**

Writing a function below to read an excel sheet into a dataframe, given the parameters `filepath` and `sheet_name`. The default for the `sheet_name` parameter is `NULL`, in case I want to use the function on an excel with only one sheet. 

I decided to use a function to read in the trash wheels' data to minimize errors by avoiding redundant code and to standardize the loading/cleaning process across all trash wheels' excel sheets.

```{r read_excel_sheet_function}
sheet_to_df = function(filepath, sheet_name=NULL){
  output = read_excel(
                    path = filepath,
                    sheet = sheet_name,
                    na = c("NA", " ", ""),
                    skip = 1,
                    trim_ws = TRUE
                    ) |> 
    janitor::clean_names() |> #clean column names
    select(where(~ any(!is.na(.)))) #remove any columns with only NA values
  
  return(output)
}
```


I'm choosing to handle the alteration, removal, and addition of specific columns outside of the `sheet_to_df` function to keep the function as minimal and generalizable as possible.

```{r load_trash_wheels_data, message = FALSE}
# Assigning trash wheel excel filepath to variable
# since it will be used repeatedly
trash_excel_filepath = "data/202309 Trash Wheel Collection Data.xlsx"

# Mr. Trash Wheel
mr_trash_df = sheet_to_df(trash_excel_filepath, "Mr. Trash Wheel") |>
  select(-homes_powered) |> #remove non-dumpster-related columns
  mutate(
    sports_balls = as.integer(sports_balls), #round sports balls column
    year = as.numeric(year), #ensure the year data type is dbl
    wheel_name = "Mr. Trash Wheel" #add name column
    )

# Professor Trash Wheel
professor_trash_df = sheet_to_df(trash_excel_filepath, "Professor Trash Wheel") |>
  select(-homes_powered) |> #remove non-dumpster-related columns
  mutate(
    wheel_name = "Professor Trash Wheel" #add name column
  )

# Gwynnda Trash Wheel
gwynnda_trash_df = sheet_to_df(trash_excel_filepath, "Gwynnda Trash Wheel") |>
  select(-homes_powered) |> #remove non-dumpster-related columns
  mutate(
    wheel_name = "Gwynnda Trash Wheel" #add name column
  )

```


**Combining the Trash Wheel dataframes.**
```{r combine_trash_dfs}
trash_wheels_df = 
  bind_rows(mr_trash_df, professor_trash_df, gwynnda_trash_df) |> 
  relocate(wheel_name) #put wheel name column at the front
```


**Answering questions with data.**
```{r}
# Number of total observations
n_total_observations = nrow(trash_wheels_df)

# Number of observations from each wheel
n_mr_trash_observations = trash_wheels_df |> 
  filter(wheel_name == "Mr. Trash Wheel") |> 
  nrow()

n_professor_observations = trash_wheels_df |> 
  filter(wheel_name == "Professor Trash Wheel") |> 
  nrow()

n_gwynnda_observations = trash_wheels_df |> 
  filter(wheel_name == "Gwynnda Trash Wheel") |> 
  nrow()

#

trash_wheels_df |> colnames()

mr_trash_df |>  colnames()
gwynnda_trash_df |> colnames()
professor_trash_df |> colnames()

trash_wheels_df |> 
  group_by(wheel_name) |> 
  summarise(
    sum_vol = sum(volume_cubic_yards, na.rm = TRUE),
    sum_weight = sum(weight_tons, na.rm = TRUE),
    max_vol = max(volume_cubic_yards, na.rm = TRUE),
    max_weight = max(weight_tons, na.rm = TRUE)
  )


```



The resulting dataset contains **`r nrow(trash_wheels_df)`** observations--





# Problem 3
### _Great British Bake Off_

**Loading and cleaning the data.**

Information about individual bakers, their bakes, and their performance is included in `bakers.csv`, `bakes.csv`, and `results.csv`. The following code chunks import these three datasets, cleans and organizes them, and merges them into a single dataset containing all the information.

First, let's import the datasets.

```{r import_gbb_data, message = FALSE}
# Bakers data
bakers_df =
  read_csv("data/gbb_datasets/bakers.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()

# Bakes data
bakes_df =
  read_csv("data/gbb_datasets/bakes.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()

# Results data
results_df =
  read_csv("data/gbb_datasets/results.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE,
           skip = 2) |> 
  janitor::clean_names()
```


Now let's take a quick look at each dataframe.
```{r view_gbb_dfs, eval = FALSE}
# View each table
bakers_df |> view()
bakes_df |> view()
results_df |> view()
```

**Explore `NA` values.**

Let's explore any `NA` values in the three dataframes.
```{r explore_gbb_na, eval = FALSE}
# Are there NA values in bakers_df?
bakers_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()

# Are there NA values in bakes_df?
bakes_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()

# Are there NA values in results_df?
results_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()
```

Based on the outputs of the code chunk above, we can see that `bakers_df` does not have any `NA` values. The `bakes_df` has `NA` values in the `show_stopper` column, and I'm not sure why. And `results_df` has hundreds of `NA` values throughout the `technical` and `results` columns, but I assume this is because the contestant was eliminated in a previous episode.

**Pre-join QAQC for `bakes_df` and `results_df`.**

To better understand the `NA` values in `bakes_df` and `results_df`, let's join the two together. Hopefully, some of the puzzle pieces will come together. First, let's do a little QA/QC to make sure the join will go smoothly.
```{r qaqc_bakes_results_1}
# Are there any bakers in bakes_df that are not in results_df?
bakes_df |> 
  anti_join(results_df, by = c("series", "episode", "baker")) |> 
  distinct(baker) |> 
  pull(baker)
```

There is one baker, "Jo", who is in `bakes_df` but not `results_df`. After some googling, I see that "Jo" in `bakes_df` is the same person as "Joanne" in `results_df` and "Jo Wheatly" in `bakers_df`. Let's sort that out and make sure she's "Joanne Wheatly" across all datasets.
```{r qaqc_joanne}
# Mutate bakers_df
bakers_df = bakers_df |> 
  mutate(
    baker_name = recode(baker_name, "Jo Wheatly" = "Joanne Wheatly")
  )

# Mutate bakes_df
bakes_df = bakes_df |> 
  mutate(
    baker = recode(baker, "\"Jo\"" = "Joanne")
  )

# No need to mutate results_df
```

Let's continue the QA/QC.
```{r qaqc_bakes_results_2}
# Are there any bakers in results_df that are not in bakes_df?
results_df |> 
  anti_join(bakes_df, by = c("series", "episode", "baker")) |> 
  distinct(series, baker) |> 
  pull(baker)
```

Yikes! There are a lot of bakers in `results_df` that do not have a match with rows in `bakes_df`. I'll take another look.
```{r qaqc_bakes_results_3, eval = FALSE}
results_df |> 
  anti_join(bakes_df, by = c("series", "episode", "baker")) |> 
  view()
```

The list of names grows by each episode, starting from episode 2. It looks many (but not all) of these names are of contestants who were previously eliminated in the show. Let's move forward with joining joining `bakes_df` and `results_df` and filter out unneeded rows from there as we see fit.
```{r join_bakes_results}
bakes_results_df = bakes_df |> 
  full_join(results_df, by = c("series", "episode", "baker")) |>
  arrange(series, baker, episode)
```

First, let's filter out any rows that indicate that a contestant has been previously eliminated. 
```{r}
bakes_results_df = bakes_results_df |> 
  filter(
    !(is.na(signature_bake) & is.na(show_stopper) & is.na(technical) & is.na(result))
  )
```






```{r}
# Assign missing bakers' names to a list
missing_bakes = results_df |> 
  full_join(bakes_df, by = c("series", "episode", "baker")) |> 
  distinct(series, baker) |> 
  arrange(series, baker)

# Filter only results for those names
bakes_results_df |> 
  filter(baker %in% pull(missing_bakes, baker)) |> 
  view()

```










**Prep `bakers_df` for joining with `bakes_results_df`.**

In `bakes_df` and `results_df`, the bakers' names are stored as just their first names. In `bakers_df`, the bakers' _full_ names are saved in one column, so I'll separate that `baker_name` column into two separate columns for first name and last name. This will make it easier to join `bakers_df` with the other dataframes in the future.
```{r clean_bakers}
# Replace baker_name column with baker_firstname and baker_lastname columns
bakers_df = bakers_df |> 
  mutate(
    baker_firstname = str_split(baker_name, " ", simplify = TRUE)[, 1],
    baker_lastname = str_split(baker_name, " ", simplify = TRUE)[, 2]
  ) |> 
  select(-baker_name) |> 
  relocate(baker_firstname, baker_lastname)
```






```{r, eval = FALSE}


# Quality Control

# Every baker must either be out, or must become the runner-up or winner





# How many distinct numbers of "words" are in the bakers' names?
  # If there are any bakers with more than two "words" in their names, str_split() may not work the way I want
bakers_df |> 
  mutate(
    test = str_count(baker_name, " ")
  ) |> 
  distinct(test)

# How many episodes are documented in the bakes table?
bakes_df |> 
  distinct(series, episode) |> 
  nrow()









bakers_df |> colnames()
bakes_df |> colnames()
results_df |> colnames()




```











```{r import_viewers_data, eval = FALSE}


viewers_df = read_csv("data/gbb_datasets/viewers.csv") |> 
  janitor::clean_names()


viewers_df |> colnames()


viewers_df |> 
  pivot_longer(
    
  )

```





