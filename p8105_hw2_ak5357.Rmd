---
title: "Data Science Homework 2"
author: "ak5357"
date: "2024-09-26"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(dplyr)
```

# Problem 1
### _NYC Transit Data_

**Loading and cleaning the data, keeping only the following variables:**

* Line
* Station Name
* Station Latitude and Longitude
* Routes Served
* Entry
* Vending
* Entrance Type
* ADA Compliance

```{r import_subway_data, message = FALSE}
transit_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", #load data
           na = c("NA", " ", "")) |> #define possible NA values
  janitor::clean_names() |> #standardize column names
  select(line, starts_with("station"), -station_location, #select focus columns
         starts_with("route"), entry, vending, entrance_type, ada) |> 
  mutate(
    entry = case_match( #convert entry column from char to lgl
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE,
    )) |> 
  mutate( #ensure all route columns are char type for consistency
    across(
      starts_with("route"), as.character
    ))
```

The dataset contains information related to each entrance and exit for each subway station in New York City. (Since this question doesn't count for points, I will forego further explanation about the dataset's variables. Sorry <3) The data are not tidy, since there is significant redundance among the columns, specifically those related to the routes.

**Answering questions with data.**
```{r data_questions}
# How many distinct stations?
num_distinct_stations = transit_df |> 
  distinct(line, station_name) |> 
  nrow()

# How many ADA stations?
num_ada_entr_ext = transit_df |>
  filter(ada == TRUE) |> 
  nrow()

# How many entrances/exits without vending and with entry?
p_no_vending_yes_entry = transit_df |> 
  summarize(
    proportion = mean(vending == "NO" & entry == TRUE)
    ) |> 
  round(4)
```

Here are some interesting facts about the data:

* The dataset contains information about **`r num_distinct_stations`** distinct stations.
* Of the `r nrow(transit_df)` entrances and exists documented in the dataset, only **`r num_ada_entr_ext`** are ADA compliant.
* The proportion of station entrances and exits without vending that allow entrance is **`r p_no_vending_yes_entry`**, or **`r p_no_vending_yes_entry * 100`**%.

**Reformatting data so that route number and route name are distinct variables.**
```{r reformat_df_by_route}
transit_dr_by_route = transit_df |> 
  pivot_longer(
    cols = starts_with("route"),
    names_to = "route_number",
    values_to = "route_name",
    names_prefix = "route"
  ) |> 
  filter(!is.na(route_name))
```


**Answering more questions with data.**
```{r _distinct_A_stations}
# How many distinct stations serve the A-line?
n_distinct_A_stations = transit_dr_by_route |> 
  filter(route_name == "A")  |> 
  distinct(line, station_name) |> 
  nrow()

# How many A-line stations are ADA compliant?
n_ada_A_stations = transit_dr_by_route |> 
  filter(route_name == "A" & ada == TRUE) |> 
  distinct(line, station_name) |> 
  nrow()
```

Here are some more interesting facts about the data:

* There are **`r n_distinct_A_stations`** distinct stations that serve the A train.
* Of the stations that serve the A train, only **`r n_ada_A_stations`** are ADA compliant.


# Problem 2
### _Mr. Trash Wheel_

**Loading and cleaning the data.**

Writing a function below to read an excel sheet into a dataframe, given the parameters `filepath` and `sheet_name`. Hopefully this will minimize errors by avoiding redundant code and standardize the loading/cleaning process across all trash wheels' excel sheets.

```{r read_excel_sheet_function}
sheet_to_df = function(filepath, sheet_name=NULL){
  output = read_excel(
                    path = filepath,
                    sheet = sheet_name,
                    na = c("NA", " ", ""),
                    skip = 1,
                    trim_ws = TRUE
                    ) |> 
    janitor::clean_names() |> #clean column names
    select(where(~ any(!is.na(.)))) #remove any columns with only NA values
  return(output)
}
```


I'm choosing to handle the alteration, removal, and addition of specific columns outside of the `sheet_to_df` function to keep the function as minimal and generalizable as possible.
```{r load_trash_wheels_data, message = FALSE}
# Assigning trash wheel excel filepath string to a variable since it's repeated
trash_excel_filepath = "data/202409 Trash Wheel Collection Data.xlsx"

# Mr. Trash Wheel
mr_trash_df = sheet_to_df(trash_excel_filepath, "Mr. Trash Wheel") |>
  mutate(
    sports_balls = as.integer(sports_balls), #round sports balls column
    year = as.numeric(year), #ensure the year data type is dbl
    wheel_name = "Mr. Trash Wheel" #add name column
    )

# Professor Trash Wheel
professor_trash_df = sheet_to_df(trash_excel_filepath, "Professor Trash Wheel") |> 
  filter(dumpster != 119) |> #removing extra row at the bottom
  mutate(wheel_name = "Professor Trash Wheel") #add name column

# Gwynnda Trash Wheel
gwynnda_trash_df = sheet_to_df(trash_excel_filepath, "Gwynnda Trash Wheel") |>
  mutate(wheel_name = "Gwynnda Trash Wheel") #add name column
```

**Combining the Trash Wheel dataframes.**

The following code chunk combines the three trash dataframes, while performing the following operations as well:

* removing empty and "total" rows that came from the bottom of the original spreadsheets
* correcting the data types of `dumpster` and `wheel_name` to numeric and factor, respectively
* re-ordering the columns in a useful way, prioritizing the variables in this order: identifiers, temporal, quantifiers, other

```{r combine_trash_dfs}
# Combine all trash wheels data
trash_wheels_df = 
  bind_rows(mr_trash_df, professor_trash_df, gwynnda_trash_df) |>
  filter(!is.na(dumpster)) |> #removing extra rows at the bottom
  mutate(
    dumpster = as.numeric(dumpster),
    wheel_name = as.factor(wheel_name)
  ) |> 
  relocate( #reorder variables
    wheel_name, dumpster, #wheel & dumpster identifiers
    year, month, date, #time-related
    weight_tons, volume_cubic_yards #quantity-related
    )
```


**Summarizing the data.**
```{r trash_n_observations, message = FALSE}
# Number of total observations
n_total_observations = nrow(trash_wheels_df)

# Number of observations from each wheel
n_mr_trash_observations = trash_wheels_df |> 
  filter(wheel_name == "Mr. Trash Wheel") |> 
  nrow()

n_professor_observations = trash_wheels_df |> 
  filter(wheel_name == "Professor Trash Wheel") |> 
  nrow()

n_gwynnda_observations = trash_wheels_df |> 
  filter(wheel_name == "Gwynnda Trash Wheel") |> 
  nrow()
```

The resulting dataset contains **`r nrow(trash_wheels_df)`** observations. Of these, **`r n_mr_trash_observations`** are from Mr. Trash Wheel, **`r n_professor_observations`** are from Professor Trash Wheel, and **`r n_gwynnda_observations`** are from Gwynnda Trash Wheel. The table below summarizes the following key variables by trash wheel:

* **Date of first collection:**
  * Earliest record for each trash wheel, `first_date`
* **Dumpsters visited:**
  * Count of distinct dumpsters were visited by each wheel, `dumpster_count`
* **Volume (cubic yards):**
  * Total volume collected, `total_vol`
  * Max volume collected at any one instance, `max_vol`
* **Weight (tons):**
  * Total weight collected, `total_weight`
  * Max weight collected at any one instance, `max_weight`

```{r trash_summary_key_variables}
# Summary table of key variables
trash_wheels_df |> 
  group_by(wheel_name) |> 
  summarise(
    first_date = min(date, na.rm = TRUE),
    dumpster_count = n_distinct(dumpster, na.rm = TRUE),
    total_vol = sum(volume_cubic_yards, na.rm = TRUE),
    max_vol = max(volume_cubic_yards, na.rm = TRUE),
    total_weight = sum(weight_tons, na.rm = TRUE),
    max_weight = max(weight_tons, na.rm = TRUE)
  ) |> 
  arrange(first_date)
```

Other variables included in the dataset are litter types (ie. plastic bottles, polystyrene, cigarette butts, glass bottles, plastic bags, wrappers, sports balls) and homes powered. The following summary table shows sums of each litter type collected and sums of homes powered.

```{r trash_summary_other_variables}
# Summary table of other variables
trash_wheels_df |> 
  group_by(wheel_name) |> 
  summarise(
    first_date = min(date, na.rm = TRUE),
    total_plastic_bottles = sum(plastic_bottles, na.rm = TRUE),
    total_polystyrene = sum(polystyrene, na.rm = TRUE),
    total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE),
    total_glass_bottles = sum(glass_bottles, na.rm = TRUE),
    total_plastic_bags = sum(plastic_bags, na.rm = TRUE),
    total_wrappers = sum(wrappers, na.rm = TRUE),
    total_sports_balls = sum(sports_balls, na.rm = TRUE),
    total_homes_powered = sum(homes_powered, na.rm = TRUE)
  ) |> 
  arrange(first_date) |> 
  select(-first_date)
```

**Answering questions with data.**
```{r trash_questions}
prof_trash_total_weight = trash_wheels_df |> 
  filter(wheel_name == "Professor Trash Wheel") |> 
  select(weight_tons) |> 
  sum(na.rm = TRUE)

gwynnda_june_2022_cigs = trash_wheels_df |> 
  filter(wheel_name == "Gwynnda Trash Wheel",
         month == "June",
         year == 2022
  ) |> 
  select(cigarette_butts) |> 
  sum(na.rm = TRUE)
```

Here are some fun facts about the data:

* In total, Professor Trash Wheel collected **`r prof_trash_total_weight`** tons of trash.
* In June 2022, Gwynnda Trash Wheel collected **`r format(gwynnda_june_2022_cigs, big.mark = ",")`** cigarette butts.

# Problem 3
### _Great British Bake Off_

**Loading the data.**

Information about individual bakers, their bakes, and their performance is included in `bakers.csv`, `bakes.csv`, and `results.csv`. The following code chunks import these three datasets, cleans and organizes them, and merges them into a single dataset containing all the information.

First, let's import the datasets.
```{r import_gbb_data, message = FALSE}
# Bakers data
bakers_df =
  read_csv("data/gbb_datasets/bakers.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()

# Bakes data
bakes_df =
  read_csv("data/gbb_datasets/bakes.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()

# Results data
results_df =
  read_csv("data/gbb_datasets/results.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE,
           skip = 2) |> 
  janitor::clean_names()
```


Now let's take a quick look at each dataframe.
```{r view_gbb_dfs, eval = FALSE}
# View each table
bakers_df |> view()
bakes_df |> view()
results_df |> view()
```

**Cleaning the data.**

Let's explore any `NA` values in the three dataframes.
```{r explore_gbb_na, eval = FALSE}
# Are there NA values in bakers_df?
bakers_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()

# Are there NA values in bakes_df?
bakes_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()

# Are there NA values in results_df?
results_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()
```

Based on the outputs of the code chunk above, we can see that

* `bakers_df` does not have any `NA` values
* `bakes_df` has `NA` values in the `show_stopper` column
* `results_df` has hundreds of `NA` values throughout the `technical` and `results` columns (possibly reflecting the null scores of previously eliminated contestants)

**QAQC for `bakes_df` and `results_df`.**

Before joining the three datasets, let's do some QA/QC to ensure the joins will go smoothly. First, let's compare the data in `bakes_df` and `results_df`.
```{r qaqc_bakes_results_1}
# Are there any bakers in bakes_df that are not in results_df?
bakes_df |> 
  anti_join(results_df, by = c("series", "episode", "baker")) |> 
  distinct(baker) |> 
  pull(baker)
```

It appears the contestant _"Jo"_ is present in `bakes_df` but not `results_df`. After some Googling, I see that _"Jo"_ in `bakes_df` is the same person as _Joanne_ in `results_df` and _Jo Wheatly_ in `bakers_df`. Let's standardize her name across all datasets to be _Joanne Wheatly_.
```{r qaqc_joanne}
# Mutate bakers_df
bakers_df = bakers_df |> 
  mutate(baker_name = recode(baker_name, "Jo Wheatly" = "Joanne Wheatly"))

# Mutate bakes_df
bakes_df = bakes_df |> 
  mutate(baker = recode(baker, "\"Jo\"" = "Joanne"))

# No need to mutate results_df
```

Let's continue the QA/QC.
```{r qaqc_bakes_results_2}
# Are there any bakers in results_df that are not in bakes_df?
results_df |> 
  anti_join(bakes_df, by = c("series", "episode", "baker")) |> 
  distinct(series, baker) |> 
  pull(baker)
```

There are many bakers in `results_df` that do not have a match with rows in `bakes_df`. After looking closer using the `view()` function, it appears that many (but not all) of these names are of previously eliminated contestants. Let's move forward with joining joining `bakes_df` and `results_df` and filter out unnecessary rows (null results of eliminated contestants) from there.
```{r join_bakes_results}
bakes_results_df = bakes_df |> 
  full_join(results_df, by = c("series", "episode", "baker")) |> 
  filter(
    !(is.na(signature_bake) & is.na(show_stopper) & is.na(technical) & is.na(result))
  ) |>
  arrange(series, baker, episode)

bakes_results_df |> head(10)
```

There are still some rows with missing data. Upon examining the output of the code chunk below, it seems there are two sources for these `NA` values.
```{r view_bakes_results_nas, eval = FALSE}
bakes_results_df |> 
  filter(if_any(everything(), is.na)) |> 
  arrange(series, episode) |> 
  view()
```

1. Series 1-8: There are some `NA` values here and there, wherever there is missing data for the `signature_bake` or `show_stopper` columns.
2. Series 9-10: There is no data at all for the `signature_bake` or `show_stopper` columns.
```{r bakes_results_series_difference}
# bakes_df only documents Series 1-8
bakes_df |> 
  distinct(series) |> 
  pull(series)

# results_df documents series 1-10
results_df |> 
  distinct(series) |> 
  pull(series)
```

Both of these missing data causes are out of my control, so let's move on with joining the datasets.

_Side note: For further QA/QC, I could check to make sure each baker has a singular show-ending (just one instance of "OUT", "Runner-up", or "WINNER") and that the episode where they have that result is the last one in their respective series where they have non-`NA` values. However, if I were to discover errors there, those would also be somewhat out of my control, so I don't see a point in going down that road._

**Prep `bakers_df` for joining with `bakes_results_df`.**

In `bakes_df` and `results_df` (now joined as `bakes_results_df`), the bakers' names are stored as just their first names. In `bakers_df`, the bakers' _full_ names are saved in a single column, so let's separate that `baker_name` column into two separate columns for first name and last name. This will make it easier to join `bakers_df` with `bakes_results_df` in the future.
```{r clean_bakers}
# Replace baker_name column with baker_firstname and baker_lastname columns
bakers_df = bakers_df |> 
  mutate(
    baker_firstname = str_split(baker_name, " ", simplify = TRUE)[, 1],
    baker_lastname = str_split(baker_name, " ", simplify = TRUE)[, 2]
  ) |> 
  select(-baker_name) |> 
  relocate(baker_firstname, baker_lastname)
```

**Finish joining the three datasets.**

I'm choosing to join the datasets in this order because I want the contestants' personal information columns to be at the end, since the focus here is on their baking.
```{r join_all_gbb}
gbb_df = bakes_results_df |> 
  full_join(bakers_df, by = c("series", "baker" = "baker_firstname")) |>
  rename(baker_firstname = baker) |> 
  arrange(series, baker_firstname, baker_lastname, episode) |> 
  relocate(series, episode, baker_firstname, baker_lastname)
```

**Export Final Dataset**
```{r}
# Export csv
write_csv(gbb_df, "data/gbb_datasets/gbb_processed_dataset.csv")
```


**Process summary.**

That was a long process. In my final code, I would have done it as shown below, all in one go.
```{r gbb_data_wrangling_summary, eval = FALSE}
# Import Bakers data
bakers_df =
  read_csv("data/gbb_datasets/bakers.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()|> 
  mutate( #add firstname and lastname columns & fix Joanne naming issue
    baker_firstname = str_split(baker_name, " ", simplify = TRUE)[, 1],
    baker_lastname = str_split(baker_name, " ", simplify = TRUE)[, 2],
    baker_name = recode(baker_name, "Jo Wheatly" = "Joanne Wheatly")
  ) |> 
  select(-baker_name) |> #remove redundant fullname column
  relocate(baker_firstname, baker_lastname) #place name columns at front

# Import Bakes data
bakes_df =
  read_csv("data/gbb_datasets/bakes.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names() |> 
  mutate(baker = recode(baker, "\"Jo\"" = "Joanne")) #fix Joanne naming issue

# Import Results data
results_df =
  read_csv("data/gbb_datasets/results.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE,
           skip = 2) |> 
  janitor::clean_names()

# Join dataframes
gbb_df = bakes_df |> 
  full_join(results_df, by = c("series", "episode", "baker")) |> 
  filter( #remove all rows that represent previously eliminated contestants
    !(is.na(signature_bake) & is.na(show_stopper) & is.na(technical) & is.na(result))
  ) |> 
  full_join(bakers_df, by = c("series", "baker" = "baker_firstname")) |>
  rename(baker_firstname = baker) |> 
  arrange(series, baker_firstname, baker_lastname, episode) |> 
  relocate(series, episode, baker_firstname, baker_lastname)

# Export csv
write_csv(gbb_df, "data/gbb_datasets/gbb_processed_dataset.csv")
```

**Answering questions with data.**

```{r}
repeat_names = gbb_df |> 
  distinct(series, baker_firstname) |> 
  group_by(baker_firstname) |> 
  summarise(count = n()) |> 
  filter(count > 1)
```

```{r}
gbb_summary_df = 0


gbb_df |> 
  mutate(
    baker_fullname = paste(baker_firstname, baker_lastname),
    series_episode = paste(series, episode)
    ) |> 
  relocate(series_episode, baker_fullname) |> 
  group_by(series) |> 
  summarise(
    star_baker = paste(baker_fullname[which(result == "STAR BAKER")], "STAR BAKER"),
    winner = paste(baker_fullname[which(result == "WINNER")], "WINNER")
  ) |> 
  view()
  
  
gbb_df |> 
  select(result) |> 
  distinct()

```










**Process viewership data.**

The following code chunk will import, clean, tidy, and organize the viewership data in `viewers.csv`.
```{r import_viewers_data, message = FALSE}
# Import, clean, and tidy viewership data
viewers_df = read_csv("data/gbb_datasets/viewers.csv") |> 
  janitor::clean_names() |> 
  pivot_longer( #tidy data from redundant series columns
    cols = starts_with("series"),
    names_to = "series",
    values_to = "rating",
    names_prefix = "series_"
  ) |> 
  mutate(series = as.numeric(series)) |> 
  relocate(series) |> 
  arrange(series, episode) |> #sort by series and episode
  filter(!is.na(rating)) #filter out non-existent episodes
```


```{r}
# Show the first 10 rows of viewers_df
viewers_df |> head(10)
```

**Answering questions with data.**
```{r gbb_view_summary}
# Viewership summary table
view_summary = viewers_df |> 
  group_by(series) |> 
  summarise(
    average_rating = mean(rating),
    highest_episode = episode[which.max(rating)],
    highest_rating = max(rating)
  )

# View summary table
view_summary

# Series 1 average viewership
s1_avg_viewership = view_summary |> 
  filter(series == 1) |> 
  pull(average_rating)

# Series 5 average viewership
s5_avg_viewership = view_summary |> 
  filter(series == 5) |> 
  pull(average_rating)
```

The table above shows the average viewership, highest-rated episode, and highest rating received by each season. The average viewership in Season 1 was `r s1_avg_viewership`. The average viewership in Season 5 was `r s5_avg_viewership`.
