Data Science Homework 2
================
ak5357
2024-09-26

# Problem 1

### *NYC Transit Data*

**Loading and cleaning the data, keeping only the following variables:**

- Line
- Station Name
- Station Latitude and Longitude
- Routes Served
- Entry
- Vending
- Entrance Type
- ADA Compliance

``` r
transit_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", #load data
           na = c("NA", " ", "")) |> #define possible NA values
  janitor::clean_names() |> #standardize column names
  select(line, starts_with("station"), -station_location, #select focus columns
         starts_with("route"), entry, vending, entrance_type, ada) |> 
  mutate(
    entry = case_match( #convert entry column from char to lgl
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE,
    )) |> 
  mutate( #ensure all route columns are char type for consistency
    across(
      starts_with("route"), as.character
    ))
```

The dataset contains information related to each entrance and exit for
each subway station in New York City.

(Dear Reader: After much consideration, I have come to a difficult
decision… Since this question doesn’t count for points, I will forego
further explanation about the dataset’s variables. My sincere apologies.
*tips hat*)

The data are not tidy, since there is significant redundance among the
columns, specifically those related to the routes.

**Answering questions with data.**

``` r
num_distinct_stations = transit_df |> 
  distinct(line, station_name) |> 
  nrow()

num_ada_stations = transit_df |> 
  filter(ada == TRUE) |> 
  nrow()

p_no_vending_yes_entry = transit_df |> 
  summarize(
    proportion = mean(vending == "NO" & entry == TRUE)
    ) |> 
  round(4)
```

Here are some interesting facts about the data:

- The dataset contains information about **465** distinct stations.
- Of the 1868 entrances and exists documented in the dataset, only
  **468** are ADA compliant.
- The proportion of station entrances / exits without vending that allow
  entrance is **0.0369**, or **3.69**%.

**Reformatting data so that route number and route name are distinct
variables.**

``` r
transit_dr_by_route = transit_df |> 
  pivot_longer(
    cols = starts_with("route"),
    names_to = "route_number",
    values_to = "route_name",
    names_prefix = "route"
  ) |> 
  filter(!is.na(route_name))
```

**Answering more questions with data.**

``` r
n_distinct_A_stations = transit_dr_by_route |> 
  filter(route_name == "A")  |> 
  distinct(line, station_name) |> 
  nrow()

n_ada_A_stations = transit_dr_by_route |> 
  filter(route_name == "A" & ada == TRUE) |> 
  distinct(line, station_name) |> 
  nrow()
```

Here are some more interesting facts about the data:

- There are **60** distinct stations that serve the A train.
- Of the stations that serve the A train, only **17** are ADA compliant.

# Problem 2

### *Mr. Trash Wheel*

**Loading and cleaning the data.**

Writing a function below to read an excel sheet into a dataframe, given
the parameters `filepath` and `sheet_name`. The default for the
`sheet_name` parameter is `NULL`, in case I want to use the function on
an excel with only one sheet.

I decided to use a function to read in the trash wheels’ data to
minimize errors by avoiding redundant code and to standardize the
loading/cleaning process across all trash wheels’ excel sheets.

``` r
sheet_to_df = function(filepath, sheet_name=NULL){
  output = read_excel(
                    path = filepath,
                    sheet = sheet_name,
                    na = c("NA", " ", ""),
                    skip = 1,
                    trim_ws = TRUE
                    ) |> 
    janitor::clean_names() |> #clean column names
    select(where(~ any(!is.na(.)))) #remove any columns with only NA values
  
  return(output)
}
```

I’m choosing to handle the alteration, removal, and addition of specific
columns outside of the `sheet_to_df` function to keep the function as
minimal and generalizable as possible.

``` r
# Assigning trash wheel excel filepath to variable, since it will be used repeatedly
trash_excel_filepath = "data/202309 Trash Wheel Collection Data.xlsx"

# Mr. Trash Wheel
mr_trash_df = sheet_to_df(trash_excel_filepath, "Mr. Trash Wheel") |>
  select(-homes_powered) |> #remove non-dumpster-related columns
  mutate(
    sports_balls = as.integer(sports_balls), #round sports balls column
    year = as.numeric(year), #ensure the year data type is dbl
    wheel_name = "Mr. Trash Wheel" #add name column
    )

# Professor Trash Wheel
professor_trash_df = sheet_to_df(trash_excel_filepath, "Professor Trash Wheel") |>
  select(-homes_powered) |> #remove non-dumpster-related columns
  mutate(
    wheel_name = "Professor Trash Wheel" #add name column
  )

# Gwynnda Trash Wheel
gwynnda_trash_df = sheet_to_df(trash_excel_filepath, "Gwynnda Trash Wheel") |>
  select(-homes_powered) |> #remove non-dumpster-related columns
  mutate(
    wheel_name = "Gwynnda Trash Wheel" #add name column
  )
```

**Combining the Trash Wheel dataframes.**

``` r
trash_wheels_df = 
  bind_rows(mr_trash_df, professor_trash_df, gwynnda_trash_df) |> 
  relocate(wheel_name)
```

**Answering questions with data.**

``` r
# Number of total observations
n_total_observations = nrow(trash_wheels_df)

# Number of observations from each wheel
n_mr_trash_observations = trash_wheels_df |> 
  filter(wheel_name == "Mr. Trash Wheel") |> 
  nrow()

n_professor_observations = trash_wheels_df |> 
  filter(wheel_name == "Professor Trash Wheel") |> 
  nrow()

n_gwynnda_observations = trash_wheels_df |> 
  filter(wheel_name == "Gwynnda Trash Wheel") |> 
  nrow()

#

trash_wheels_df |> colnames()
```

    ##  [1] "wheel_name"         "dumpster"           "month"             
    ##  [4] "year"               "date"               "weight_tons"       
    ##  [7] "volume_cubic_yards" "plastic_bottles"    "polystyrene"       
    ## [10] "cigarette_butts"    "glass_bottles"      "plastic_bags"      
    ## [13] "wrappers"           "sports_balls"

``` r
mr_trash_df |>  colnames()
```

    ##  [1] "dumpster"           "month"              "year"              
    ##  [4] "date"               "weight_tons"        "volume_cubic_yards"
    ##  [7] "plastic_bottles"    "polystyrene"        "cigarette_butts"   
    ## [10] "glass_bottles"      "plastic_bags"       "wrappers"          
    ## [13] "sports_balls"       "wheel_name"

``` r
gwynnda_trash_df |> colnames()
```

    ##  [1] "dumpster"           "month"              "year"              
    ##  [4] "date"               "weight_tons"        "volume_cubic_yards"
    ##  [7] "plastic_bottles"    "polystyrene"        "cigarette_butts"   
    ## [10] "plastic_bags"       "wrappers"           "wheel_name"

``` r
professor_trash_df |> colnames()
```

    ##  [1] "dumpster"           "month"              "year"              
    ##  [4] "date"               "weight_tons"        "volume_cubic_yards"
    ##  [7] "plastic_bottles"    "polystyrene"        "cigarette_butts"   
    ## [10] "glass_bottles"      "plastic_bags"       "wrappers"          
    ## [13] "wheel_name"

``` r
trash_wheels_df |> 
  group_by(wheel_name) |> 
  summarise(
    sum_vol = sum(volume_cubic_yards, na.rm = TRUE),
    sum_weight = sum(weight_tons, na.rm = TRUE),
    max_vol = max(volume_cubic_yards, na.rm = TRUE),
    max_weight = max(weight_tons, na.rm = TRUE)
  )
```

    ## # A tibble: 3 × 5
    ##   wheel_name            sum_vol sum_weight max_vol max_weight
    ##   <chr>                   <dbl>      <dbl>   <dbl>      <dbl>
    ## 1 Gwynnda Trash Wheel      4618       903.    2309       452.
    ## 2 Mr. Trash Wheel         17868      3750.    8934      1875.
    ## 3 Professor Trash Wheel    3092       433.    1546       216.

The resulting dataset contains **849** observations–

# Problem 3

### *Great British Bake Off*

**Loading and cleaning the data.**

Information about individual bakers, their bakes, and their performance
is included in `bakers.csv`, `bakes.csv`, and `results.csv`. The
following code chunks import these three datasets, cleans and organizes
them, and merges them into a single dataset containing all the
information.

First, let’s import the datasets.

``` r
# Bakers data
bakers_df =
  read_csv("data/gbb_datasets/bakers.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Bakes data
bakes_df =
  read_csv("data/gbb_datasets/bakes.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Results data
results_df =
  read_csv("data/gbb_datasets/results.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE,
           skip = 2) |> 
  janitor::clean_names()
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

First, let’s take a quick look at each dataframe.

``` r
# View each table
bakers_df |> view()
bakes_df |> view()
results_df |> view()
```

In `bakes_df` and `results_df`, the bakers’ names are stored as just
their first names. In `bakers_df`, the bakers’ *full* names are saved in
one column, so I’ll separate that `baker_name` column into two separate
columns for first name and last name. This will make it easier to join
`bakers_df` with the other dataframes in the future.

``` r
# Replace baker_name column with baker_firstname and baker_lastname columns
bakers_df = bakers_df |> 
  mutate(
    baker_firstname = str_split(baker_name, " ", simplify = TRUE)[, 1],
    baker_lastname = str_split(baker_name, " ", simplify = TRUE)[, 2]
  ) |> 
  select(-baker_name) |> 
  relocate(baker_firstname, baker_lastname)
```

Let’s explore any `NA` values in the three dataframes.

``` r
# The bakers_df does not have any NA values
bakers_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()

# The bakes_df has some NA values in the show_stopper column
bakes_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()

# The results_df has many NA values in the technical and results columns
results_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()
```

Based on the outputs of the code chunk above, we can see that
`bakers_df` does not have any `NA` values, `bakes_df` has `NA`s in the
`show_stopper` column, and `results_df` has hundreds of `NA`s throughout
the `technical` and `results` columns

``` r
# Are there any bakers in bakes_df that are not in results_df?
bakes_df |> 
  anti_join(results_df, by = c("series", "episode", "baker")) |> 
  distinct(baker)

# Are there any bakers in results_df that are not in bakes_df?
results_df |> 
  anti_join(bakes_df, by = c("series", "episode", "baker")) |> 
  distinct(series, baker)
```

``` r
bakes_df |> 
  full_join(results_df, by = c("series", "episode", "baker")) |>
  arrange(series, baker, episode) |> 
  view()
```

``` r
  mutate(
    baker = recode(baker, "\"Jo\"" = "Joanne")
  )

# Quality Control

# Every baker must either be out, or must become the runner-up or winner





# How many distinct numbers of "words" are in the bakers' names?
  # If there are any bakers with more than two "words" in their names, str_split() may not work the way I want
bakers_df |> 
  mutate(
    test = str_count(baker_name, " ")
  ) |> 
  distinct(test)

# How many episodes are documented in the bakes table?
bakes_df |> 
  distinct(series, episode) |> 
  nrow()









bakers_df |> colnames()
bakes_df |> colnames()
results_df |> colnames()
```

``` r
viewers_df = read_csv("data/gbb_datasets/viewers.csv") |> 
  janitor::clean_names()


viewers_df |> colnames()


viewers_df |> 
  pivot_longer(
    
  )
```
