Data Science Homework 2
================
ak5357
2024-09-26

# Problem 1

### *NYC Transit Data*

**Loading and cleaning the data, keeping only the following variables:**

- Line
- Station Name
- Station Latitude and Longitude
- Routes Served
- Entry
- Vending
- Entrance Type
- ADA Compliance

``` r
transit_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", #load data
           na = c("NA", " ", "")) |> #define possible NA values
  janitor::clean_names() |> #standardize column names
  select(line, starts_with("station"), -station_location, #select focus columns
         starts_with("route"), entry, vending, entrance_type, ada) |> 
  mutate(
    entry = case_match( #convert entry column from char to lgl
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE,
    )) |> 
  mutate( #ensure all route columns are char type for consistency
    across(
      starts_with("route"), as.character
    ))
```

The dataset contains information related to each entrance and exit for
each subway station in New York City. (Since this question doesn’t count
for points, I will forego further explanation about the dataset’s
variables. Sorry \<3) The data are not tidy, since there is significant
redundance among the columns, specifically those related to the routes.

**Answering questions with data.**

``` r
# How many distinct stations?
num_distinct_stations = transit_df |> 
  distinct(line, station_name) |> 
  nrow()

# How many ADA stations?
num_ada_entr_ext = transit_df |>
  filter(ada == TRUE) |> 
  nrow()

# How many entrances/exits without vending and with entry?
p_no_vending_yes_entry = transit_df |> 
  summarize(
    proportion = mean(vending == "NO" & entry == TRUE)
    ) |> 
  round(4)
```

Here are some interesting facts about the data:

- The dataset contains information about **465** distinct stations.
- Of the 1868 entrances and exists documented in the dataset, only
  **468** are ADA compliant.
- The proportion of station entrances and exits without vending that
  allow entrance is **0.0369**, or **3.69**%.

**Reformatting data so that route number and route name are distinct
variables.**

``` r
transit_dr_by_route = transit_df |> 
  pivot_longer(
    cols = starts_with("route"),
    names_to = "route_number",
    values_to = "route_name",
    names_prefix = "route"
  ) |> 
  filter(!is.na(route_name))
```

**Answering more questions with data.**

``` r
# How many distinct stations serve the A-line?
n_distinct_A_stations = transit_dr_by_route |> 
  filter(route_name == "A")  |> 
  distinct(line, station_name) |> 
  nrow()

# How many A-line stations are ADA compliant?
n_ada_A_stations = transit_dr_by_route |> 
  filter(route_name == "A" & ada == TRUE) |> 
  distinct(line, station_name) |> 
  nrow()
```

Here are some more interesting facts about the data:

- There are **60** distinct stations that serve the A train.
- Of the stations that serve the A train, only **17** are ADA compliant.

# Problem 2

### *Mr. Trash Wheel*

**Loading and cleaning the data.**

Writing a function below to read an excel sheet into a dataframe, given
the parameters `filepath` and `sheet_name`. The default for the
`sheet_name` parameter is `NULL`, in case I want to use the function on
an excel with only one sheet.

I decided to use a function to read in the trash wheels’ data to
minimize errors by avoiding redundant code and to standardize the
loading/cleaning process across all trash wheels’ excel sheets.

``` r
sheet_to_df = function(filepath, sheet_name=NULL){
  output = read_excel(
                    path = filepath,
                    sheet = sheet_name,
                    na = c("NA", " ", ""),
                    skip = 1,
                    trim_ws = TRUE
                    ) |> 
    janitor::clean_names() |> #clean column names
    select(where(~ any(!is.na(.)))) #remove any columns with only NA values
  
  return(output)
}
```

I’m choosing to handle the alteration, removal, and addition of specific
columns outside of the `sheet_to_df` function to keep the function as
minimal and generalizable as possible.

``` r
# Assigning trash wheel excel filepath to variable
# since it will be used repeatedly
trash_excel_filepath = "data/202309 Trash Wheel Collection Data.xlsx"

# Mr. Trash Wheel
mr_trash_df = sheet_to_df(trash_excel_filepath, "Mr. Trash Wheel") |>
  select(-homes_powered) |> #remove non-dumpster-related columns
  mutate(
    sports_balls = as.integer(sports_balls), #round sports balls column
    year = as.numeric(year), #ensure the year data type is dbl
    wheel_name = "Mr. Trash Wheel" #add name column
    )

# Professor Trash Wheel
professor_trash_df = sheet_to_df(trash_excel_filepath, "Professor Trash Wheel") |>
  select(-homes_powered) |> #remove non-dumpster-related columns
  mutate(
    wheel_name = "Professor Trash Wheel" #add name column
  )

# Gwynnda Trash Wheel
gwynnda_trash_df = sheet_to_df(trash_excel_filepath, "Gwynnda Trash Wheel") |>
  select(-homes_powered) |> #remove non-dumpster-related columns
  mutate(
    wheel_name = "Gwynnda Trash Wheel" #add name column
  )
```

**Combining the Trash Wheel dataframes.**

``` r
trash_wheels_df = 
  bind_rows(mr_trash_df, professor_trash_df, gwynnda_trash_df) |> 
  relocate(wheel_name) #put wheel name column at the front
```

**Answering questions with data.**

``` r
# Number of total observations
n_total_observations = nrow(trash_wheels_df)

# Number of observations from each wheel
n_mr_trash_observations = trash_wheels_df |> 
  filter(wheel_name == "Mr. Trash Wheel") |> 
  nrow()

n_professor_observations = trash_wheels_df |> 
  filter(wheel_name == "Professor Trash Wheel") |> 
  nrow()

n_gwynnda_observations = trash_wheels_df |> 
  filter(wheel_name == "Gwynnda Trash Wheel") |> 
  nrow()

#

trash_wheels_df |> colnames()
```

    ##  [1] "wheel_name"         "dumpster"           "month"             
    ##  [4] "year"               "date"               "weight_tons"       
    ##  [7] "volume_cubic_yards" "plastic_bottles"    "polystyrene"       
    ## [10] "cigarette_butts"    "glass_bottles"      "plastic_bags"      
    ## [13] "wrappers"           "sports_balls"

``` r
mr_trash_df |>  colnames()
```

    ##  [1] "dumpster"           "month"              "year"              
    ##  [4] "date"               "weight_tons"        "volume_cubic_yards"
    ##  [7] "plastic_bottles"    "polystyrene"        "cigarette_butts"   
    ## [10] "glass_bottles"      "plastic_bags"       "wrappers"          
    ## [13] "sports_balls"       "wheel_name"

``` r
gwynnda_trash_df |> colnames()
```

    ##  [1] "dumpster"           "month"              "year"              
    ##  [4] "date"               "weight_tons"        "volume_cubic_yards"
    ##  [7] "plastic_bottles"    "polystyrene"        "cigarette_butts"   
    ## [10] "plastic_bags"       "wrappers"           "wheel_name"

``` r
professor_trash_df |> colnames()
```

    ##  [1] "dumpster"           "month"              "year"              
    ##  [4] "date"               "weight_tons"        "volume_cubic_yards"
    ##  [7] "plastic_bottles"    "polystyrene"        "cigarette_butts"   
    ## [10] "glass_bottles"      "plastic_bags"       "wrappers"          
    ## [13] "wheel_name"

``` r
trash_wheels_df |> 
  group_by(wheel_name) |> 
  summarise(
    sum_vol = sum(volume_cubic_yards, na.rm = TRUE),
    sum_weight = sum(weight_tons, na.rm = TRUE),
    max_vol = max(volume_cubic_yards, na.rm = TRUE),
    max_weight = max(weight_tons, na.rm = TRUE)
  )
```

    ## # A tibble: 3 × 5
    ##   wheel_name            sum_vol sum_weight max_vol max_weight
    ##   <chr>                   <dbl>      <dbl>   <dbl>      <dbl>
    ## 1 Gwynnda Trash Wheel      4618       903.    2309       452.
    ## 2 Mr. Trash Wheel         17868      3750.    8934      1875.
    ## 3 Professor Trash Wheel    3092       433.    1546       216.

The resulting dataset contains **849** observations–

# Problem 3

### *Great British Bake Off*

**Loading and cleaning the data.**

Information about individual bakers, their bakes, and their performance
is included in `bakers.csv`, `bakes.csv`, and `results.csv`. The
following code chunks import these three datasets, cleans and organizes
them, and merges them into a single dataset containing all the
information.

First, let’s import the datasets.

``` r
# Bakers data
bakers_df =
  read_csv("data/gbb_datasets/bakers.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()

# Bakes data
bakes_df =
  read_csv("data/gbb_datasets/bakes.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE) |> 
  janitor::clean_names()

# Results data
results_df =
  read_csv("data/gbb_datasets/results.csv",
           na = c("", " ", "N/A", "NA", "UNKNOWN"),
           trim_ws = TRUE,
           skip = 2) |> 
  janitor::clean_names()
```

Now let’s take a quick look at each dataframe.

``` r
# View each table
bakers_df |> view()
bakes_df |> view()
results_df |> view()
```

**Explore `NA` values.**

Let’s explore any `NA` values in the three dataframes.

``` r
# Are there NA values in bakers_df?
bakers_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()

# Are there NA values in bakes_df?
bakes_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()

# Are there NA values in results_df?
results_df |> 
  filter(if_any(everything(), is.na)) |> 
  view()
```

Based on the outputs of the code chunk above, we can see that
`bakers_df` does not have any `NA` values. The `bakes_df` has `NA`
values in the `show_stopper` column, and I’m not sure why. And
`results_df` has hundreds of `NA` values throughout the `technical` and
`results` columns, but I assume this is because the contestant was
eliminated in a previous episode.

**Pre-join QAQC for `bakes_df` and `results_df`.**

To better understand the `NA` values in `bakes_df` and `results_df`,
let’s join the two together. Hopefully, some of the puzzle pieces will
come together. First, let’s do a little QA/QC to make sure the join will
go smoothly.

``` r
# Are there any bakers in bakes_df that are not in results_df?
bakes_df |> 
  anti_join(results_df, by = c("series", "episode", "baker")) |> 
  distinct(baker) |> 
  pull(baker)
```

    ## [1] "\"Jo\""

There is one baker, “Jo”, who is in `bakes_df` but not `results_df`.
After some googling, I see that “Jo” in `bakes_df` is the same person as
“Joanne” in `results_df` and “Jo Wheatly” in `bakers_df`. Let’s sort
that out and make sure she’s “Joanne Wheatly” across all datasets.

``` r
# Mutate bakers_df
bakers_df = bakers_df |> 
  mutate(
    baker_name = recode(baker_name, "Jo Wheatly" = "Joanne Wheatly")
  )

# Mutate bakes_df
bakes_df = bakes_df |> 
  mutate(
    baker = recode(baker, "\"Jo\"" = "Joanne")
  )

# No need to mutate results_df
```

Let’s continue the QA/QC.

``` r
# Are there any bakers in results_df that are not in bakes_df?
results_df |> 
  anti_join(bakes_df, by = c("series", "episode", "baker")) |> 
  distinct(series, baker) |> 
  pull(baker)
```

    ##  [1] "Lea"        "Mark"       "Annetha"    "Louise"     "Jonathan"  
    ##  [6] "David"      "Jasminder"  "Keith"      "Simon"      "Ian"       
    ## [11] "Urvashi"    "Ben"        "Jason"      "Robert"     "Yasmin"    
    ## [16] "Janet"      "Natasha"    "Peter"      "Victoria"   "Stuart"    
    ## [21] "Manisha"    "Ryan"       "Sarah-Jane" "Cathryn"    "Danny"     
    ## [26] "Toby"       "Lucy"       "Deborah"    "Mark"       "Ali"       
    ## [31] "Robert"     "Howard"     "Glenn"      "Christine"  "Beca"      
    ## [36] "Claire"     "Enwezor"    "Jordan"     "Diana"      "Iain"      
    ## [41] "Norman"     "Kate"       "Martha"     "Chetna"     "Stu"       
    ## [46] "Marie"      "Dorret"     "Sandy"      "Ugnė"       "Alvin"     
    ## [51] "Mat"        "Paul"       "Flora"      "Lee"        "Louise"    
    ## [56] "Michael"    "Kate"       "Val"        "Rav"        "Tom"       
    ## [61] "Benjamina"  "Selasi"     "Peter"      "Chris"      "Flo"       
    ## [66] "Tom"        "James"      "Julia"      "Yan"        "Liam"      
    ## [71] "Stacey"     "Antony"     "Briony"     "Dan"        "Jon"       
    ## [76] "Karen"      "Kim-Joy"    "Luke"       "Rahul"      "Ruby"      
    ## [81] "Terry"      "Imelda"     "Manon"      "Alice"      "Amelia"    
    ## [86] "David"      "Helena"     "Henry"      "Jamie"      "Michael"   
    ## [91] "Phil"       "Priya"      "Rosie"      "Steph"      "Dan"       
    ## [96] "Michelle"

Yikes! There are a lot of bakers in `results_df` that are not in
`bakes_df`. Let’s take another look.

``` r
results_df |> 
  anti_join(bakes_df, by = c("series", "episode", "baker")) |> 
  distinct(series, baker) |> 
  distinct(series) |> 
  arrange(series) |> 
  pull(series)
```

    ##  [1]  1  2  3  4  5  6  7  8  9 10

All 10 series? Then perhaps it is not an error, but a systemic pattern.
Let’s go ahead with joining `bakes_df` and `results_df` and find out
what’s going on.

``` r
bakes_results_df = bakes_df |> 
  full_join(results_df, by = c("series", "episode", "baker")) |>
  arrange(series, baker, episode)
```

``` r
# Assign missing bakers' names to a list
missing_bakes = results_df |> 
  anti_join(bakes_df, by = c("series", "episode", "baker")) |> 
  distinct(series, baker) |> 
  arrange(series, baker)

# Filter only results for those names
bakes_results_df |> 
  filter(baker %in% pull(missing_bakes, baker)) |> 
  view()
```

**Prep `bakers_df` for joining with `bakes_results_df`.**

In `bakes_df` and `results_df`, the bakers’ names are stored as just
their first names. In `bakers_df`, the bakers’ *full* names are saved in
one column, so I’ll separate that `baker_name` column into two separate
columns for first name and last name. This will make it easier to join
`bakers_df` with the other dataframes in the future.

``` r
# Replace baker_name column with baker_firstname and baker_lastname columns
bakers_df = bakers_df |> 
  mutate(
    baker_firstname = str_split(baker_name, " ", simplify = TRUE)[, 1],
    baker_lastname = str_split(baker_name, " ", simplify = TRUE)[, 2]
  ) |> 
  select(-baker_name) |> 
  relocate(baker_firstname, baker_lastname)
```

``` r
# Quality Control

# Every baker must either be out, or must become the runner-up or winner





# How many distinct numbers of "words" are in the bakers' names?
  # If there are any bakers with more than two "words" in their names, str_split() may not work the way I want
bakers_df |> 
  mutate(
    test = str_count(baker_name, " ")
  ) |> 
  distinct(test)

# How many episodes are documented in the bakes table?
bakes_df |> 
  distinct(series, episode) |> 
  nrow()









bakers_df |> colnames()
bakes_df |> colnames()
results_df |> colnames()
```

``` r
viewers_df = read_csv("data/gbb_datasets/viewers.csv") |> 
  janitor::clean_names()


viewers_df |> colnames()


viewers_df |> 
  pivot_longer(
    
  )
```
